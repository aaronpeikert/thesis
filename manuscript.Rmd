---
title: "Towards Transparency and Open Science"
subtitle: "A Principled Perspective on Computational Reproducibility and Preregistration"
output:
  bookdown::pdf_document2:
    latex_engine: "xelatex"
    toc: true
    number_sections: false
header-includes: |
  \usepackage{libertine}
  \usepackage{pdfpages}
  \usepackage{titling}
  \pretitle{
     \begin{minipage}{\textwidth}
     \includepdf[offset=5mm 0mm]{cover.pdf}
     \end{minipage}
     \newpage
     \clearpage%
     \thispagestyle{empty}%
     \addtocounter{page}{-2}%
     \null%
     \clearpage
     \begin{center}\LARGE}
  \posttitle{\end{center}}
  
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
  \setotherlanguage[]{german}
fontsize: 12pt
linestretch: 1.2
geometry: "left=3.6cm, right=3.6cm, top=4cm, bottom=4cm, bindingoffset=5mm"
classoption: twoside
papersize: a4
bibliography: references.bib
abstract: "`r paste0(c('\\\\setstretch{1.0}English:', readLines('abstract.md'), '\\\\par Deutsch:\\\\textgerman{', readLines('zusammenfassung.md'), '}\\\\newpage'), collapse = ' ')`"
csl: apa.csl
toc: false # it is manually placed
repro:
  files:
    - abstract.md
    - apa.csl
    - papers/
  tlmgr:
    - pgf
    - preview
    - libertine
    - pdfpages
  apt:
    - git
    - rsync
---

\newpage
\clearpage
\thispagestyle{empty}
\null
\clearpage

\begin{spacing}{1}
\tableofcontents
\end{spacing}

\newpage
\clearpage
\thispagestyle{empty}
\null
\clearpage

# Introduction

<!--psychology has unique difficulties-->
Psychology is a difficult science [@meehlTheoreticalRisksTabular1978].
Although there is some disagreement on why exactly this is the case, I doubt there is disagreement about the fact itself.
Laying open the difficulties and attempting to overcome them are not a recent trends, though they have been invigorated by the so-called replication crisis in psychology [@ioannidisWhyMostPublished2005; @opensciencecollaborationEstimatingReproducibilityPsychological2015] that also begins to ripple through other empirical sciences.
As psychology grapples with this crisis of confidence in its empirical results, several causes and remedies have been proposed.
We can roughly divide the proposed countermeasures into two categories, those that aim to increase the correct use of statistical methods [e.g., @bakanTestSignificancePsychological20060327; @benjaminRedefineStatisticalSignificance2018; @cohenEarth051994;  @gigerenzerMindlessStatistics2004; @wagenmakersWhyPsychologistsMust2011] and those that aim to counteract sociological and psychological biases [e.g., @simmonsFalsepositivePsychologyUndisclosed2011; @rosenthalFileDrawerProblem1979; @bakkerRulesGameCalled2012; @johnMeasuringPrevalenceQuestionable2012].

<!--bla-transparency-bla-->
In my view, both categories try to address a lack of transparency about the inductive process in the empirical test of a theory.
Testing a theory empirically is often viewed as deductive, since the theory is making statements about the data.
Empirical scientists, however, often simultaneously engage in induction by deriving general statements from data.
I will argue that the inductive element in the empirical test leads to overconfidence in the empirical results, if unaccounted.
The extent of this bias depends on the inductive process.
The inductive process therefore have to be transparent to other researcher because they are otherwise unable to  judge the empirical support of a theory.

The above distinction, between statistical and sociological countermeasures, arises arises from two different kinds of induction.
On the one hand, we have inductive processes that are well defined in the form of statistical methods, on the other hand researcher also engage in more informal inductive behavior outside of well defined models.
They require different countermeasures because statistical methods make the inductive bias quantifiable; while open science measures reduce some uncertainty about the remaining informal sources of inductive bias.
To understand why transparency is crucial, we must understand that formal and informal induction are integral to empirical sciences.
Any science must be able to communicate how it generates its knowledge.
However, transparency has an outstanding role in psychology and other empirical sciences, because empirical statements lose their value without transparency about the inductive process that contributed to them.
Therefore, transparency is more than a virtue that may improve knowledge gains in empirical sciences but is an indispensable property.

<!--induction is necessary for psychological science-->
To function as an empirical science, psychology must be able to make statements about the world that can be compared to the actual conditions of the world.
In psychology, this is not a purely deductive endeavor.
Very few psychological theories are precise enough to derive testable statements.
While it is tempting to claim that a theory makes deductively testable claims by, e.g., implying a mean difference between two groups [@leePopperFalsificationCorroboration2021], such a statement is not testable on its own.
Consider the example of mean difference between a placebo and an experimental group, that we test using a simple t-test of mean differences.
The decision to reject the null hypothesis (no mean difference between groups) still depends, besides true mean difference and sample size, on the observed variance.
However, the variance needs to be estimated from the data.
So the threshold of the deductive decision depends on a quantity that must be induced.

<!--sometimes the use of induction is well justified-->
Induction is necessary and, in the present case, innocuous for psychology as a science.
Here, it is innocuous because the introduced bias can be accounted for and vanishes with increasing sample size, but we will see later that this is not always the case.
A t-test accounts for the estimation of variance by having somewhat wider tails than a z-test which assumes that the variance is known, i.e., the hurdle to reach significance is higher for a t-test (the critical value for a one sided test with α-level set to 1% and degrees of freedom set to 10, is t≈`r round(qt(.99, 10), 2)` vs. z≈`r round(qnorm(.99), 2)`).
It is widely known that even without this correction, the z-test is a good approximation when sample sizes are large [@studentProbableErrorMean1908].
Induction is also necessary because it is virtually impossible to ask psychological researchers to specify every detail, such as the variance, a priori from their theory.
If they had to, there would probably be no psychological theory that could be subject of an empirical test.

<!--necessity of induction-->
In other words, induction gives our theories some slack to be imprecise and contain "blank" spaces, later to be filled based on observation.
It allows researchers to concentrate on the essential statements of their theories and choose some of the assumptions to fit the data well.
In some ways, it is the empirical researchers' answer to the Duhem–Quine problem [@duhemPhysicalTheoryExperiment1976; @vanormanquineTwoDogmasEmpiricism1976], which states that any empirical test of a theory is testing the conjunction of theory, auxiliary assumptions, and conjectures [@meehlTheoreticalRisksTabular1978; @meehlAppraisingAmendingTheories1990].
By inducing some quantities, psychological researchers can remove them from the conjunction.
If researchers use induction for some necessary but under the theory arbitrary assumptions, their theory will not be refuted because of these assumptions.
Since empirical researchers often cannot derive every assumption from their theory, avoiding refutation because of those assumptions is a desirable property.

<!--statistical induction can be dangerous-->
By the same token, whole theories may escape refutation by replacing every ill-fitting statement deduced from theory with statements induced from data.
If applied to auxiliary assumptions, such strategy of post hoc changing a theory in light of facts has been called "Lakatosian Defense" [@meehlAppraisingAmendingTheories1990].
If pushed to the limit, we arrive at a "theory" governed by the data.
Such a theory, full of empirically induced statements, is almost empty of statements that have been empirically verified.
The data used for induction can not refute these statements, so they they never have been subject of an empierical test.

<!--the appraisal of "induced theories" should be low-->
So what to think of such, yet untested, theory?
Researchers and philosophers of science differ considerably in their opinion about how to appraise theories, e.g., judging the long-term performance (if they are frequentists), degrees of belief (if they are Bayesians), or probativeness [if they are severe testers, @mayoStatisticalInferenceSevere2018, p. 14] of a hypothesis.
Whatever measure they subscribe to, they would agree on a low appraisal of an untested theory.

<!--transparency about induction is required-->
So empirical researchers find themselves in a pinch.
On the one hand, they need induction to test their imprecise theories.
On the other hand, induction may render any test of a hypothesis ineffective.
The problem, I argue, therefore, is not induction but making transparent where and to what extend induction is used in the inferential process.
The replication crisis can be traced to a misjudgment of how much induction has been going on in psychology and hence, how well-tested the empirical claims as reported in the literature actually are.
Therefore, the question of this dissertation is *what* must be made transparent, and *how* to best make it transparent?

<!--forshadow the main part-->
The first question (the *what*) is theoretical in nature.
It is addressed in the synopses, which supplies the theoretical framework of the articles written as part of the dissertation.
Under this framework, induction is split into a process that can be formally analyzed (statistical methods) and a part that is much more difficult to judge (sociological factors).

<!--how it relates to the articles-->
Based on this distinction, the articles answer the second question (the *how*).
I argue that transparency about statistical methods is enabled by computational reproducibility, while transparency about sociological factors is facilitated by preregistration.
The conceptualization of computational reproducibility and preregistration as means for transparency is supplemented by practical guidance on how researchers may implement these approaches

# What necessitates transparency

The need for transparency is closely tied to the use of induction in the empirical test of a theory.
There has been a long and vigorous debate about what it means to test a theory empirically [@popperLogicScientificDiscovery2002; Ch. 5 Experience as a Method].
I do not attempt to rehash the debate about what constitutes an empirical test but aim to lay open the role of transparency in two frameworks that lend themselves to investigate induction.
The first framework motivates transparency when the aim of an empirical test is to evaluate the verisimilitude ("closeness to truth") of a theory.
The second framework motivates transparency under a science that wants to select a theory according to its expected predictive performance.

Both frameworks show how unaccounted induction leads to overconfidence in empirical results and give us some theoretical tools to asses and control this overconfidence.
Because both are quite technical, they are followed by a more conceptual summary of these tools.
These sections provide the basis to understand how computational reproducibility and preregistration enable a proper assessment of an empirical test on a conceptual level.

## An information-theoretic perspective

Information theory provides a rigorous mathematical measure that can be understood as the verisimilitude of a theory.
We can formalize the distance to the truth in terms of how much information about the truth is lost when the theory is used to model reality.
Expressed mathematically, assume the existence of a function $f(x)$ that yields the likelihood to observe the state of the world $x$ where $f$ represents the ground truth.
We are now interested in how much information is lost if we use $g(x)$, our theory, instead of $f(x)$, the reality, over all possible states $\mathcal{X}$.
Expressed as lost bits of information, a measure known as Kullback–Leibler divergence [@kullbackInformationSufficiency1951], we get:
<!--x is a single state, X is the random variable, and \mathcal{X} is all possible outcomes of X-->
\begin{align} 
\mathcal{L}_{KL}(f, g) &=\int_{\mathcal{X}}f(x)\log\left(f(x)\right)\mathrm{d}x - \int_{\mathcal{X}}f(x)\log\left(g(x)\right)\mathrm{d}x\\
&= \mathbb{E}_{X \sim f}\left[\log(f(x))\right] - \mathbb{E}_{X \sim f}\left[\log(g(x))\right]\label{eq:klexp}
\end{align}

Most readers will recognize that this information-theoretic setup and the derivation below follow closely @burnhamModelSelectionMultimodel2002, Ch. 7.2, in their derivation of the Akaike Information Criterion [@hirotuguakaikeInformationTheoryExtension1971] in its general form.
What is of interest here is not the derivation but how this conceptualization can help us to understand what happens when data is simultaneously used to induce quantities of a theory and test the theory.

This setup is, of course, highly theoretical.
$\mathcal{L}_{KL}(f, g)$ is unknowable, since the truth is unobserved.
However, this fact does not impede us from getting closer to the truth because we can still compare two theories relative to each other.
Because the expectation for $f$ remains constant (left-hand expectation in Eq. \@ref(eq:klexp)), we only need to estimate the relative expected loss of information (right-hand expectation in Eq. \@ref(eq:klexp)) to make a comparative judgment.
To make a relative judgment about several competing theories it suffices to estimate for any theory $g(x)$:
\begin{equation}
\mathbb{E}_{X \sim f}\left[\log(g(x))\right]
\end{equation}

To allow for quantities to be induced, we must assume that our theory is parameterized, e.g., $g(x|\theta)$.
That means our theory implies a family of possible probability distributions that may describe reality.
This parameterization captures the idea that some assumptions necessary for a theory to make testable statements are arbitrary.
Of those arbitrary assumptions, we want to find those that fit the reality with the least amount of information lost.
The best parameterization is achieved by:
\begin{equation}
\theta_* = \operatorname*{arg\,min}_\theta \mathcal{L}_{KL}(f, g(\cdot|\theta))
\end{equation}

The inference goal is for comparing other theories to $g(x)$ is, therefore:
\begin{equation}
\mathbb{E}_{X \sim f}\left[\log(g(x|\theta_*))\right]
\end{equation}
Of course, we usually do not know $\theta_*$.
That is why it is necessary to induce it from data, denoted as $\hat\theta(y)$, where $Y$ are $n$ independent samples from $X\sim f$.

The crucial point here is to understand what happens when we can not derive $\theta$ deductively but must substitute it inductively with an estimate $\hat\theta(y)$.
Any estimated parameters $\hat{\theta}(y)$ would almost surely not be equal to $\theta_*$ (assuming $\theta$ may take an infinite number of values, i.e., is continues).
It follows, almost surely, that we loose information:
\begin{equation}
\mathcal{L}_{KL}(f, g(\cdot|\hat{\theta}(y)) > \mathcal{L}_{KL}(f, g(\cdot|\theta_*))
\end{equation}
Or ignoring the constant:
\begin{equation}
\mathbb{E}_{X \sim f}\left[\log(g(x|\theta_*))\right] >\mathbb{E}_{X \sim f}[\log(g(x|\hat{\theta}(y)))] 
\end{equation}

That is to say, any induced estimate will be sub-optimal.
The inference goal, however, is to compare the theory $g$ to reality $f$, not to evaluate the estimates of $\hat\theta$.
The point is to make a statement about the theory, not to make a statement about the data in combination with the theory.
If the estimate of $\hat\theta(y)$, i.e., the inductive process, is unbiased in the sense that it converges towards $\theta_*$, we may form an expectation over the data $Y$:
\begin{equation}
\mathbb{E}_{Y \sim f^n}\mathbb{E}_{X \sim f}[log(g(x|\hat\theta(y)))]
\end{equation}

Forming this expectation over data is a crucial step; it requires us to think beyond the data we observed to all the data we could have observed.
There are two ways to get at this expectation.
One is the use of Taylor series expansion, which follows in this section, and another is the use of cross-validation discussed in the next section.

We usually favor procedures that promise unbiased estimates for the observed distance on the data used to induce $\hat \theta$, given that their assumptions are met.
With slight abuse of notation, $log(g(y|\theta)) \equiv \sum_{i = 1}^n log(g(y_i|\theta))$, so that $\mathbb{E}_{y \sim f^n}log(g(y|\hat\theta(y)))$ refers to the observed likelihood.
The observed likelihood is often easy enough to obtain, e.g. in maximum likelihood estimation.

The expectation over the data together with Taylor series expansion yields:
\begin{equation}
\mathbb{E}_{Y \sim f^n}\mathbb{E}_{X \sim f}[log(g(x|\hat\theta(y)))] \approx \mathbb{E}_{Y \sim f^n}[\log(g(y|\hat{\theta}(y)))] - tr[J(\theta_*)I(\theta_*)^{-1}]
\end{equation}

Where $J$ is the Fisher information matrix with regard to $g$, and $I$ for $f$ respectively.
For more details about this derivation see @burnhamModelSelectionMultimodel2002, Ch. 7.2.

The observed likelihood $log(g(x|\hat\theta(y)))$ is, therefore, a biased estimate of the distance to the truth.
We may conclude that substituting deduced quantities by induced estimates leads to some overconfidence about how close one is to the truth.
This overconfidence is directly related to how much induction a model entails.
This bias is often called the complexity or capacity of a model, i.e., how much the data is influencing the results and, hence, how detailed the model is representing the data [@goodfellowCapacityOverfittingUnderfitting2016].
I therefore denote it as as $\mathcal{C}$.
\begin{equation}
\mathbb{E}_{X \sim f}\mathbb{E}_{Y \sim f^n}[log(g(x|\hat\theta(y)))] \approx \mathbb{E}_{Y \sim f^n}[\log(g(y|\hat{\theta}(y)))] + \mathcal{C}
\end{equation}

If we want to induce quantities and correctly appraise a theory on the same data, we must know how much we have to correct our appraisal for the induction.
Fortunately, it is possible to approximate the complexity of a model under some conditions.
One condition is, since $\theta_*$ is unknown, that we know the properties of the inductive process that generated $\hat\theta$.
We then can formally analyze the behavior and derive a mathematical expression for $\mathcal{C}$, e.g., corrections for a large class of statistical models, most famously the class of linear models, are well known [@boisbunonAkaikeInformationCriterion2014].

## A future performance perspective

In addition to closeness to truth, there is another line of argumentation about why transparency about the process of induction is important.
Instead of verisimilitude, one might be concerned with future performance.
That is, how well does a theory do in predicting novel facts.
Please note that the information-theoretic setup above has made no appeal to the expected performance on unseen data.
Verisimilitude and expected performance are different motivations for transparency, though they can be linked.
In the future performance setup, we do not appeal to ground truth, replace the Kullback–Leibler divergence with an arbitrary loss function, and no longer require $g(x)$ to return a likelihood ($\mathcal{L}$ stands for a arbitrary loss function):
\begin{equation}
\mathcal{L}(x, g(x|\theta)) = \mathbb{E}_xL(x, g(x|\theta))
\end{equation}

Again we can define the loss we have observed in the sample used to estimate $\hat\theta$ [@sochLawUnconsciousStatistician2020]:
\begin{equation}
\mathbb{E}_y L(y, g(y|\hat\theta(y)))
 = \frac{1}{n}\sum_{i=1}^n L(y_i, g(y_i|\theta))
\end{equation}

However, what we are interested in is not how well the theory did on data that informed it but on future, yet unseen, data:
\begin{equation}
\mathbb{E}_x\mathbb{E}_yL(x, g(x|\hat\theta(y)))
\end{equation}
This expectation over, what is often called training and test data, is termed generalization error or expected prediction error [@bengioNoUnbiasedEstimator2004] and resembles the expectation over data discussed in the information-theoretic setup [@stoneAsymptoticEquivalenceChoice1977].

Instead of using the Taylor series expansion, we can repeatedly sample data and repeat the inductive process.
That is, we make use of cross-validation where the data is partitioned, and the inductive process is repeated on all permutations of a subset of the partitions.
For each subset, the resulting model is then compared to the complement that was not used for induction, which is indicated by $y_{-i} = y \diagdown \{y_i\}$.
\begin{equation}
\mathbb{E}_{x}\mathbb{E}_{y}  L(x, g(x|\hat\theta(y)))
 = \frac{1}{n}\sum_{i=1}^{n}L(y_i, g(y_i|\hat\theta(y_{-i})))
\end{equation}

As stated earlier, using cross-validation it is possible to  estimate \linebreak[4] $\mathbb{E}_{X \sim f}\mathbb{E}_{Y \sim f^n}[log(g(x|\hat\theta(y)))]$ as well; which connects the information-theoretic setup with this approach [@stoneAsymptoticEquivalenceChoice1977; @stoneCrossValidatoryChoiceAssessment1974].
To make the link to the first approach even more clear, we may define complexity in these terms [@hauensteinComputingAICBlackbox2018]:
\begin{equation}
\mathcal{C} = \frac{1}{n} \sum_{i=1}^{n}L(y_i, g(y_i|\hat\theta(y_{-i}))) - L(y_i, g(y_i|\hat\theta))
\end{equation}

Instead of a formal analysis to derive $\mathcal{C}$, we can simply repeat the inductive process, i.e., using cross-validation.
That drastically expands the set of inductive processes for which we can estimate the inductive bias since it is considerably easier to repeat the process than to derive the complexity analytically.

## A conceptual perspective

Now that we have established the need for transparency about the inductive process, we can drop a few of the more technical details to get a more straightforward about what we have to make transparent.
It bears repeating that simply laying open what has been done is not enough.
Merely showing the inductive results instead of the process that generated them is insufficient to appraise the theory.
On a conceptual level, we want to compare:
\begin{equation}
\mathcal{L}(\text{Theory}, \text{Reality})
\end{equation}

Where, $\mathcal{L}$ stands for loss function, i.e., how to compare predictions and reality.
To allow for induction to happen, we replace theory with a model (not necessarily a statistical one) or, put differently, a multitude of implications about the data from the theory.

\begin{equation}
\mathcal{L}(\text{Model(Reality)}, \text{Reality})
\end{equation}
The idea is that we choose the version of our theory that best fits reality.
However, we are forced to rely on a limited sample of reality.
We are misled because these two factors, induction and limited sample size, interact.
Choosing the best version of our theory, based on a sample, is almost surely suboptimal.
The observed loss, therefore, is an overconfident estimate of the loss in the future and closeness to truth.
\begin{equation}
\mathcal{L}(\text{Model(Reality)}, \text{Reality}) > \mathcal{L}(\text{Model(Sample)}, \text{Sample})
\end{equation}
\begin{equation}
\mathcal{L}(\text{Model(Reality)}, \text{Reality}) \approx \mathcal{L}(\text{Model(Sample)}, \text{Sample}) + \mathcal{C}
\end{equation}

Transparency is necessary because induction leaves researchers overly optimistic regarding their theories' fit to the data.
The extent of this optimism depends on the inductive process, not merely its results.
Specifically, it depends on the complexity, i.e., the ability of the inductive process to adept to data.
Without knowing the inductive process, researchers can not judge the overconfidence, so the inductive process ought to be made transparent.

# How to establish transparency

The above sections aimed to motivate the observation that the apparent fit of a theory to data is often overly optimistic, if it has inductive elements.
This observation is only of limited use if we do not now the extend of this optimism.
However, both setups show that the extent of the optimism ($\mathcal{C}$) is closely related to the inductive process and give us two starting points for making this bias transparent.
The first requires a formal analysis of the inductive process to compute the complexity (e.g., using information criteria), and the second merely requires that the process is repeatable (e.g., using cross-validation).
Both approaches require researchers to make the process of induction transparent instead of merely publishing the results.

Even a casual consideration of the above formalization should strike anyone who has ever worked with empirical data as unrealistic.
No one can actually expect researchers to be inductive only in ways that are formally analyzable or even strictly repeatable.
The point is to set the goal post and have a yardstick to measure how well a method aimed at improving empirical sciences does.

It is without question that researchers sometimes engage in inductive behavior that is neither formally analyzable nor repeatable.
This fact implies that for theses situations the optimism bias can not be fully quantified and that full transparency can not be archived.
To enable proper judgment of the whole theory, the imperative is simple: induce only what is necessary and what you induce should, if at all possible, be done formally.
Otherwise, the supposedly objective test of the theory using hard data must still be judged subjectively.

We, therefore, have two bounds that limit transparency about the inductive process.
First, some things can not be made transparent in principle because some induction happens informally, in the sense that we can not estimate the optimism with certainty.
Second, even formalized inductive processes only allow to estimate optimism theoretically but they have to be communicated effectively to do so in practice.

In the following, I propose preregistration, as means to move induction into the formal domain, and computational reproducibility to make formal induction transparent.

## Transparency about statistical models: Computational Reproducibilibiltiy

## Transparency about human researcher: Preregistration

# Discussion

## Limitations

### Theoretical

### Practical

## Future Research

# Articles

## A Reproducible Data Analysis Workflow With R Markdown, Git, Make, and Docker

The following article is reprinted from the following source under [Creative Commons Attribution (CC BY) 4.0 International License](https://creativecommons.org/licenses/by/4.0).

Peikert, A., & Brandmaier, A. M. (2021). A Reproducible Data Analysis Workflow With
R Markdown, Git, Make, and Docker. Quantitative and Computational Methods in Behavioral Sciences, 1, e3763. https://doi.org/10.5964/qcmb.3763

\includepdf[pages=-, offset=5mm 0mm,frame=false]{papers/workflow.pdf}

## Reproducible Research in R: A Tutorial on How to Do the Same Thing More Than Once

The following article is reprinted from the following source under [Creative Commons Attribution (CC BY) 4.0 International License](https://creativecommons.org/licenses/by/4.0).

Peikert, A., van Lissa, C. J., & Brandmaier, A. M. (2021). Reproducible Research in R: A Tutorial on How to Do the Same Thing More Than Once. *Psych*, *3*(4), 836–867. https://doi.org/10.3390/psych3040053

\includepdf[pages=-, offset=5mm 0mm]{papers/tutorial.pdf}

## Why does preregistration increase the persuasiveness of evidence? A Bayesian rationalization.

The following article is prepared to be submitted to *Philosophical Psychology*.
The preprint is reprinted here under [Creative Commons Public Domain Dedication (CC0 1.0)](https://creativecommons.org/publicdomain/zero/1.0/).

\includepdf[pages=-, offset=5mm 0mm]{papers/prereg.pdf}


# References
