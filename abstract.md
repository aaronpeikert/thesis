Psychology and other empirical sciences are in the middle of a crisis, as many researchers have become aware that many findings do not have as much empirical support as they once believed.
Several causes of this inflated confidence in findings have been proposed: misuse of statistical methods, sociological biases, and weak theories.
This dissertation proposes the following rationale: to some extent, imprecise theories are unavoidable, but they still can be subjected to an empirical test by employing induction.
Data may be used to amend theories, allowing precise predictions that can be compared to reality.
However, such a strategy comes at a cost.
While induction is necessary, it causes overconfidence in empirical findings.
When assessing findings this overconfidence must be taken into account.
The extent of the overconfidence depends on the properties of the inductive process.
Some inductive processes can be made fully transparent, so their bias can be accounted for appropriately.
I show that this is the case for induction that can be repeated at will on other data, highlighting the importance of computational reproducibility.
Induction involving the researcher and their cognitive model can not be repeated; hence, the extent of overconfidence must be judged with uncertainty.
I propose that reducing this uncertainty should be the objective of preregistration.
Having explicated the goals of computational reproducibility and preregistration from a perspective of transparency about induction in the synopses, I put forward recommendations for the practice of both in the articles published as part of the dissertation.
